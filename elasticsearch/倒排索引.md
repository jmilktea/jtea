前面我们详细介绍了mysql innodb的[B+树](https://github.com/jmilktea/jmilktea/blob/master/mysql/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E7%B4%A2%E5%BC%95.md)，了解了索引工作的基本原理，一般情况下B+树在维持3~5高度可以存储千万级数据，但现在互联网系统数据量动则上亿级甚至百亿级，显然无法很好的支持。   
另外我们也说到B+树对于like ‘%name%’ 左右模糊搜索是无法支持的，尽管mysql也提供了Full Text(全文索引)来支持全文搜索，但elasticsearch是个更好的选择，mysql等关系型数据的重点设计不在于搜索，而es基于lucene，专门为了搜索。   
那么在es(底层都是lucene)中是怎么支持海量数据的全文搜索呢？答案就是倒排索引，也叫反向索引。   

与mysql中类型，es中一行数据成为一个document，也有id和其它字段，假设有(id,name,content)3个字段，通过id找到文档是正向找法，而通过content的内容找到文档这种反过来的找法称为反向索引(inverted index)，国内习惯翻译为倒排索引。es是基于lucene开发库的，所以下文在分析索引的时候更多说的是lucene。      

## 概念    
维基百科   
倒排索引（英语：Inverted index），也常被称为反向索引、置入文件或反向文件，是一种索引方法，被用来存储在全文搜索下某个单词在一个文档或者一组文档中的存储位置的映射。它是文档检索系统中最常用的数据结构。    

我们以博客系统为例，一篇博客有标题和内容，如下，content的内容可能非常多    
![image](https://github.com/jmilktea/jmilktea/blob/master/elasticsearch/images/inverted-index1.png)    

在搜索的时候，用户输入"elasticsearch"，通常我们会希望把内容包含该单词的文章都搜索出来。接下来我们看下es是如何为content这个字段建立倒排索引的。    

## 原理   
当我们插入一篇文章的时候，默认情况下es会为我们每个字段建立倒排索引，为了实现全文检索，首先会对内容进行分词，关于分词我们后面单独讲。    
会content分词后会得到如下结果    
![image](https://github.com/jmilktea/jmilktea/blob/master/elasticsearch/images/inverted-index2.png)   
其中term dictionary是content分词后的结果，里面的每一项称为term，positing list从名字可以看出它是定位表的意思，例如其中保存了文档id，可以定位到文档的位置。搜索的目的就是通过term找到文档id，然后再通过文档id找到具体的文档，这有点类似于B+树中的二级索引，通过二级索引先找到主键值，再通过主键值找到行数据。    
实际情况要复杂得多，term dictionary可能会非常大，如何存储和查询呢？如果使用B+树来存储term dictionary，那么就回到mysql的路子，需要多次磁盘io。要想加快查询，本质上就是要减少磁盘io次数，尽可能把数据缓存到内存，在内存的查找速度要比磁盘随机io快几个数量级，mysql实际上也尽可能在做一点，例如B+树的根节点比较小，mysql让它常驻内存。但是term dictionary会非常大，放在内存中不合适。    

**term index**    
term index是term的索引，它的设计是为了能快速定位到term的位置。既然要放到内存中，那么term index就不能像term一样那么大，lucuene的做法是对其进行压缩，压缩到足够小就可以放到内存中去搜索了，虽然这里多了一步需要先通过term index找term，但是内存中的查找速度是非常快的，可以减少term查询的磁盘io次数。    
我们先来看一个最简单的例子，假设有字符串 “ab”,"abc","abcd","abe"，仔细看他们都有一些公共的前缀，abc,abe已经包含了ab，abcd已经包含了ab,abc。   

**trie树**   
trie树称为字典树或单词查找树，用于搜索引擎的文本搜索和统计。我们还是用算法可视化工具来看下 https://www.cs.usfca.edu/~galles/visualization/Trie.html   
![image](https://github.com/jmilktea/jmilktea/blob/master/elasticsearch/images/inverted-index3.gif)  
从图可以看到，当插入abc时，只需要插入一个内容为c的节点，当插入abcd时，只需要插入一个内容为d的节点，图中节点有颜色的表示终点，例如b是ab的终点。当我们搜索abe时，就是从根节点处理，沿着分叉查，找到abe路径并且e是终点那么就能找到abe了。    

从结构上来看term index可以认为是一颗trie树，实际没那么简单，lucene在它的基础上做了一些改进（mysql也在B+树上做了改进），如图    
![image](https://github.com/jmilktea/jmilktea/blob/master/elasticsearch/images/iverted-index4.webp)    
term index就是图中tip部分，实际上lucene会把term index保存在一个.tip文件中，每个FSTIndex就是一个term index，其叶子节点指向了tim block的位置。   
tim就是term dictionary的保存文件，对应到.tim文件中，在segement内部lucene会继续拆分为块block，这个就是term index指向的位置，接着就在block中找到目标term，block在lucene中大小大约在25~48,已经是一个很小的范围，使用二分法或跳表都可以快速遍历。       

从上图我们看到了FST关键字，[FST](https://en.wikipedia.org/wiki/Finite-state_transducer)（Finite-State-Transducer 有限状态传感器），是一种图数据结构，lucene使用它对term index进行压缩，FST的特点是占用空间小同时还能快速搜索，它不仅会对前缀进行压缩，也会对后缀进行压缩，使得空间进一步缩小。例如有字符串“stop”,"start","mart"三个字符，仔细看“stop”和“start”拥有共同的前缀“st”，“start”与"mart"拥有共同后缀"art"，FST构建后如图，[FST在线体验](http://examples.mikemccandless.com/fst.py?terms=stop%0D%0Astart%0D%0Amart%0D%0A&cmd=Build+it%21)          
![image](https://github.com/jmilktea/jmilktea/blob/master/elasticsearch/images/inverted-index5.png)  
通过上面可以了解到，lucene为了加快term dictionary的搜索速度设计了term index，并且尽可能的压缩，使得term index能载入内存，通过term index快速定位到term dictionary。   

**posting list**    
找到term dictionary并没有完成整个搜索，我们最终的目的是找到文档，所有还得通过term dictionary找到document。    
lucene会为每个文档创建一个唯一、递增的整形id，称为docid，文档id，类似于mysql行中的主键，docid在segement内是唯一的，使用整形数字的目的是为了方便压缩。还压缩，没错，lucene为了减少存储空间，很多地方都使用压缩。需要注意的是这docid和我们在es中说的文档id是两码事，docid是由lucene内部生成和使用，es的文档id是为了我们方便标识文档，可以是我们自己创建也可以由es生成的唯一字符串。    

posting list并不是我们图中简单存一个文档id，还存储了term在实际文档中出现的次数，出现的位置等，这些记录方便统计term与文档的相关度等。    
posting list最重要的就是docid，一个term可能对应多个文档，记录下来就是数组，如[1,2,3]。问题是这个数组可能会非常大，例如包含有elasticsearch词的文章可能有几十万篇，那么这个数组也会占用非常大的空间，lucene会对它进行压缩。   

**Frame of Reference（FOR）**   
简单的说，就是不记录完整的id，而是记录增量。什么意思呢？例如有10,100,140,150,155，可以记录为[10,90,40,10,5]，后面记录的是对比前一个数的增量，这样做的好处是把大数变成小数，原本要用int存储的可以变成用short存储，原本用short存储的可以用byte存储，这样就减少存储成本，对于几十万几百万的文档id，这种节省是很可观的。按照上面的说法，会不会插入一个中间的数呢，例如145，这样就打乱了后面的增量数值了。实际并不会出现这种情况，因为我们上面说到lucene的文档id是递增生成的。   
我们看下如下图    
![image](https://github.com/jmilktea/jmilktea/blob/master/elasticsearch/images/inverted-index6.png)   
原本需要24个byte存储(6个int)经过压缩后变成了7个byte，其中第二步会进行分块，每个块里面的数值使用最大那个数字的位数即可，因为最大数的位数足够表达小的数字，里面以3个为例，实际中不会这么少，分块的目的是为了能把大小数区分开来，如上如果不分只有一个快，那就需要用最大数值的位数，相当于没压缩。   
备注：227的二进制是1110 0011，30的二进制是0001 1110     

**Roaring bitmaps**   
有时候我们的查询条件不只一个，例如查询名称为“elasticsearch”并且内容有“lucene”的博客，按照上面的理解，会通过名称找到包含“elasticsearch”的文档id集合，再通过内容找到包含“lucene”的文档id集合，我们的查询条件是“并且”，那么就需要求这两个文档集合的交集。怎么求交集呢，遍历？明显不合适，如果文档的内容非常多，加载到内存会非常占用空间，而且直接比较集合求交集效率会比较低。这个时候就要用到**位图**了(位图在很多面试也非常常见，当涉及到数据量大的场景，内存空间又不足的情况下首先就可以想到位图)        

假设文档id为[1,3,5,6]，那么可以用位图[1,0,1,0,1,1]来表示，其中1表示当前下标对应的值存在，0表示不存在，用一个bit就可以表示一个数字存不存在，这样有两个好处：  
- 节省空间，假设有100M个文档id，我们只需要 100M * 1/8 byte = 12.5M 即可    
- 位运算更快，位运算可以直接“与”“或”运算，位运算的效率很高，在很多java源码中也会通过位运算来代替取模等运算，计算机对于0,1是最熟悉的     

使用位图仍存在一个问题，极端情况下，文档id为[1,1000000000]，那么位图就需要用[1,0,0,0,.....,1]，只有两个文档id确要用这么多bit才能表示，明显浪费空间。   
Roaring bitmaps是更高效的位图算法，它将文档id按照65535为界限去分块，比如第一块所包含的文档id范围在0\~65535之间，第二块的id范围是65536\~131071，以此类推。用<商，余数>表示每一个id，这样每个块内的id范围都是在0~65535范围内，不会出现上面相差很大的情况，如图：   
![image](https://github.com/jmilktea/jmilktea/blob/master/elasticsearch/images/inverted-index7.png)   
备注：1000 / 65536 = (0,1000), 131385 / 65536 = (2,313)    
之所以选择65535,是因为它是2个字节(short)能表示的最大整数。    
图中最后一段说到，块内大于4096个值，则使用bit来表示，小于4096个则用2个字节直接表示。什么意思呢？   
通过上面我们也可以看到，当值比较少的时候，使用位来表示是不划算的，例如[1,1000] 只有两个数字，用2个short类型表示就可以了，更加节省，而大于4096个才转换位图表示。   
那为什么是4096呢？这里有点绕，但实际很简单，我们推算一下。   
假设刚好有4096个值，用short来存，就需要8192个字节，往后就是每加一个数多2个字节。   
如果用位图，最极端的情况是[1,65536]，需要65536个bit，也是8192个字节，往后再加多少数，由于我们已经把最大值给表示了，所以不会再增加空间了。
我们可以参考一下网上的一张图，非常清楚    
![image](https://github.com/jmilktea/jmilktea/blob/master/elasticsearch/images/inverted-index8.png)   


**Frame of Reference 与 Roaring bitmaps 的关系**   
lucenet通过Frame of Reference压缩数据，减少磁盘的存储空间，在查询的时候，是需要把这些数据解压还原的，接着我们需要对这些结果做一些交集，并集的处理来得到最终的结果，这个时候如果全部放到内存中就会占用很高的内存资源，且无法高效的求交集，并集。   
为了更节约内存和更高效的运算，lucene 使用Roaring bitmaps是一个位图算法，将文档按65535界限去分块，然后求<商,余数>，使得每个块的数字都不会超过65535，然后再使用位图来表示每个数字，这样可以更加节约内存，也更方便与或运算。    

## 总结    
倒排索引是lucene和es中非常重要的数据结构，是实现全文搜索的基础。lucene中一个segement就是一个倒排索引，查询的本质就是在每个segement中把数据找出来然后汇总得出结果。    
term dictionary存储了所有term，它占用的空间比较大，没法完全加载到内存。term index是term dictionary的索引，是用FST压缩得到一颗空间占用较少的树，可以载入内存，通过term index可以在内存中快速定位到term的位置，减少了磁盘io。   
找到term后可以通过文件指针找到posting list，posting list在存储是使用Frame of Reference压缩，节约磁盘存储空间，在获得文档id后，需要做交集，并集处理，使用Roaring bitmaps对文档id进行压缩，进而可以载入内存，通过位进行高效运算。    
我们可以看到lucene是非常“吝啬”的，压缩在lucene中非常常见，这也是它能保持高效的重要原因。    
