## 简介    
es默认会为文档每个字段建立倒排索引，对于text类型的字段会先进行分词，然后再存储，在搜索时也会先分词，再进行查询，前面我们说到分词后是每一个term，es的全文搜索就是搜索匹配的term。    
例如：how to learn elasticsearch，可以分词为“how” "to" "learn" "elasticsearch"存储，在搜索“learn elasticsearch”时，es会先分词为“learn”和“elasticsearch”去搜。   

**text与keyword**     
这是两个可以存储string的数据类型，两者最重要的差别是keyword是不会进行分词的，es会完整保留keyword的内容，例如像姓名，我们并不需要模糊检索，那么可以设置为keyword类型，分词是有性能损耗的，而且会增加存储成本，对于不需要全文检索的字符串应该使用keyword，反之则使用text。   

es分词的过程称为analysis，是将全文文本转换为一系列单词(term/token)的过程，analysis是通过analyzer完成的，es已经内置了许多analyzer供我们使用。   
分词分为三个过程：character filters, tokenizer, toker filters，如下：  
![image](https://github.com/jmilktea/jmilktea/blob/master/elasticsearch/images/analyzer-1.png)    

character filters：字符过滤器，在分词之前对文本进行预处理，例如去掉html标签。一个分词器可以有0到多个character filters    
tokenizer：分词器，将全文文本按照一定规则分为一系列单词，一个分词器必须且只有一个tokenizer   
toker filters：分词过滤器，对分词后的单词进行过滤，例如一些停顿词a,the,in一般没什么用，可以去掉    

## 分词器    
es 内置了如下分词器    
standard analyzer：默认分词器，按词切分，大写转换为小写   
simple analyzer：按照非字母切分，大小转换为小写   
stop analyzer：大写转换为小写，停顿词过滤    
whitespace analyzer：按空格切分，不会转小写   
keyword analyzer：不分词，相当于是keyword类型   
patter analyzer：基于正则表达式，默认是按非字母切分   
language：提供多种语言分词   
自定义分词     

我们可以使用_analyze api来看下分词效果，示例：“In 10 years, Technology will change”    
```
GET _analyze
{
  "analyzer": "standard",
  "text": "In 10 years, Technology will change"
}
```
standard analyzer: in,10,years,technology,will,change    
simple：in,years,technology,will,change     
stop: years,technology,change   
从搜索的角度的来说，停顿词一般是不需要的，用户不会去搜索a,the,in这类无异于的词，而是technology这类有针对性的词    

我们也可以在创建index mapping的时候，为字段指定具体的analyzer    
```
#创建索引，每个字段指定analyzer
PUT /term_index/
{
  "mappings": {
    "properties": {
      "content1" : {
        "type": "text",
        "analyzer": "standard"
      },
      "content2":{
        "type": "text",
        "analyzer": "stop"
      }
    }
  }
}
#添加记录
POST /term_index/_doc
{
  "content1":"In 10 years, Technology will change",
  "content2":"In 10 years, Technology will change"
}
#standard分词，in可以搜索出来
GET /term_index/_search
{
  "query": {
    "match": {
      "content1": "in"
    }
  }
}
#stop分词，in不会搜索出来
GET /term_index/_search
{
  "query": {
    "match": {
      "content2": "in"
    }
  }
}
```

**中文分词**    
英文天生有空格可以拆分单词，但是中文一般是连在一起的，例如：“我爱北京天安门”，默认的分词器拆分为我，爱，北，京，天，安，门    
而如果使用stop分词器会拆分为：我爱北京天安门。这结果都不是我们希望的，es默认的分词器对中文的支持不够友好。    

支持中文的常用分词器也有很多种，比较常用的是[ik](https://github.com/medcl/elasticsearch-analysis-ik)和[HanLP](https://github.com/hankcs/HanLP)     
这些分词器不是es内置的，需要通过plugin的方式进行安装，安装完成后重新es生效。这里我们体验一下HanLp    

安装插件   
./bin/elasticsearch-plugin install https://github.com/KennFalcon/elasticsearch-analysis-hanlp/releases/download/v7.10.0/elasticsearch-analysis-hanlp-7.10.0.zip    
主要版本要和es的对应起来，安装成功后使用./bin/elasticsearch-plugin list查看安装结果           
安装后再plugins目录下就会有hanlp目录，表示插件已经成功安装，重新启动一下es   
```
#使用hanlp分词，得到结果：我，爱，北京，天安门
GET _analyze 
{
  "analyzer": "hanlp_standard",
  "text": "我爱北京天安门"
}
```
效果还是比较好的，例如北京，天安门，这些都属于一个词汇。   
在一些特点的场景可能有一些特定的词汇，例如“给力”，是网络新创的词语，默认情况下是会分为给，力，此类词语我们希望它是一个词，可以添加到自定义词典库，实际hanlp自己也维护了许多词典库。   
我们在路径/plugins/analysis-hanlp/data/dictionary/custom下新增一个my.txt，把“给力”添加进去，然后es的config目录下找到/anaylsis-hanlp/hanlp.properties，修改CustomDictionaryPath，其默认值是   
```
data/dictionary/custom/CustomDictionary.txt; ModernChineseSupplementaryWord.txt; ChinesePlaceName.txt ns; PersonalName.txt; OrganizationName.txt; ShanghaiPlaceName.txt ns;data/dictionary/person/nrf.txt nrf;  
```
用空格分隔表示是在同一个目录下，我们可以添加my.txt   
```
data/dictionary/custom/CustomDictionary.txt; my.txt; ModernChineseSupplementaryWord.txt; ChinesePlaceName.txt ns; PersonalName.txt; OrganizationName.txt; ShanghaiPlaceName.txt ns;data/dictionary/person/nrf.txt nrf;
```
启动es，可以看到给力已经分成一个词   
![image](https://github.com/jmilktea/jmilktea/blob/master/elasticsearch/images/analyzer-2.png)    

如果我们想继续新增呢？比如新增一个“奥利给”，它默认是被分成奥利，给。   
我们继续在my.txt新增“奥利给”，然后删除同目录下的CustomDictionary.txt.bin文件，稍等一会后，可以看到奥利给也被分成一个词。   
这里并不需要重启es，从es的控制台也可以看出，每分钟会去判断hanlp是否需要重新加载自定义词典。  
![image](https://github.com/jmilktea/jmilktea/blob/master/elasticsearch/images/analyzer-3.png)     

实际情况中es节点可能会很多，如果每台机器都这样维护词典是非常麻烦的，hanlp支持远程词典，这样就可以通过远程配置，由每个节点的hanlp自己去拉取更新了。   
