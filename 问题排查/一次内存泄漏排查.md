## 背景     
生产有个服务，最近不定时的抽风，内存升高，接着就是服务挂掉，也是非常典型的一个问题。    
刚开始问题暴露的不明显，有两台机器，一台只给了512m的内存，并且从业务得知最近业务服务量有增加，怀疑是内存不足，加了内存。   
运行了两三天，某个不确定的时间，服务又挂了，之所以说“某个不确定的时间”，是因为如果时间确定，那比较好定位，例如是不是定时任务，或者某个时间有第三方对系统进行访问。接着就是加内存，但结果显而易见，对于内存溢出，加内存是无底动，每次加好后确实稳定了一段时间（这个实际是服务重启暂时恢复了，后面就知道是为啥了），但时不时又会起来，其实这也是个关键的时间点，例如是否触发了某个功能，或者访问了某个数据。    

## 监控    
我们先看下tomcat的请求        
![image](1)   
可以看到在某些时间点，tomcat的线程都满了，大量的线程在排队等待，而后面再进来的线程就会被拒绝     

接着看下请求的耗时    
![image](5)   
可以看到零星有些接口耗时很高，开始我们怀疑是不是某些接口出了问题，拖垮了整个服务，但是从时间点上来说对不太上，而在服务垮掉的时候，没有说固定哪些接口就很慢。 当然从实际来说这些接口都需要优化，不过不是我们本次关注的重点。   


接着看下CPU相关的监控      
![image](3)   
可以看到在请求积压的这段时间，CPU是上升的，同时很多线程处于time waiting状态，这符合tomcat线程的表现    
CPU升高有两处地方值得思考   
1是程序用了大量的计算，例如加解密，数学运算等，这点可以通过jstack查看到具体的线程，在我们本次的排查中不是这个问题   
2是jvm gc，gc是很消费cpu资源的，特别是full gc，无论是young gc还是full gc，都需要stop the world，也就是会终止业务线程的执行    

接着我们看下jvm的相关监控    
![image](2)    
可以看到内存在某些时间段上升非常快，另外老年代占用的内存很高（其中某些直线下降是由于服务重启），正常情况下我们的对象应该都在新生代分配和消亡，新生代内存不足就会触发young gc，当多次回收不了的对象才会进入到老年代，从图中可以看到老年代内存几乎是直线上升，也就说明很多对象回收不了，当老年代满了，就会触发full gc。   

接着我们看下gc的情况   
![image](4)    
从监控可以看到gc停顿的时间非常久，蓝色那个是full gc，停顿的时间很长，说明老年代的对象很多，而且结合上一个监控看，在这段full gc的时间内，老年代的内存并没有释放多少，也就是扫描的对象很多，而且回收不了。   

## 问题定位     
遇到这种问题，我们习惯性的就是dump文件，从dump文件可以看到内存使用的详情，使用命令    
```
jmap -dump:format=b,file=./highmem.hprof pid
```
但是jmap dump文件非常慢，如果内存使用很高，dump需要一些时间，另外从服务器下载下来也非常久，非常麻烦，当dump文件很大的时候，上传到在线平台分析，或者本地打开也都非常不方便。    

**我司有一条面试题就是：生产应用发生内存泄漏，运行程序占用空间很高，在dump不下来的情况下怎么去分析、解决这个问题。**           

假设dump的文件比较小，我们可以使用在线工具分析    
[gceasy](https://www.gceasy.io/)    
[headdump](https://thread.console.heapdump.cn/) 这个的前身是你假笨，阿里出来的一位大佬做的社区产品    
本地的话可以使用mat或者jdk自带的visualvm     

在dump下来之前，我们先从最简单的方式出来，上面已经有证据指正是内存出现泄漏，那么最简单的方式就是排查程序中的静态变量和while循环。    
我们排查了所以静态变量，没有可疑的地方，接着就是while循环，有两处while(true)写法，其中的逻辑都是从第三方分页获取数据，然后放到本地的一个局部变量集合。会不会是while出现死循环，这个假设符合场景，但需要证明。我开始的做法是使用arthas监控一下调用的那个接口，分页参数会不会一直递增，例如offset参数会不会一直在变大，但是接口的调用很多，我只是想看offset很大的，那些小的都是正常的，这里使用的是arthas watch命令中的条件表达式，写法如下：    
```
watch com.xxx.xxxClient getData "{params,returnObj}" "{params[0] > 100}" -x 2 -b
```    
这里就只观察第一个参数是大于100的请求，在请求非常多，watch需要过滤的时候，条件表达式就非常有用。关于arthas的条件表达式，有时间单独写一篇来总结一下。    
监控了一段时间没有发现此类请求，要不就是分页没有问题，要不就是我们最开始提到的可能是某些场景才触发，监控时间没触发到。    

同时review的一下代码，发现一处写法有不妥的地方，简化如下    
```
List<BizBillDetailBo> list = Lists.newArrayList();
while(true) {
    List<BizBillDetailBo> datas = xxxClient.getData(params);
    //...
    datas.stream().foreach(s->{
        list.add(s);
    });
    //...
    if(datas.size() < 25) {
        break;
    }
}
```   
这里的break条件是拿到的对象集合小于25就退出，至于为什么是25，回忆了一下，跟实际业务场景有关，是最多两年(24个月)的数据，但是写法非常诡异，为bug也埋下了伏笔，当出现25个的时候就死循环了。    

**dump文件分析**          
如果dump文件太大，visualvm在操作的过程中可能会奔溃，可以加多内存，找到电脑上的visualvm.conf文件，调整 -xmx 参数，例如2g。        
![image](6)   
打开后就非常明显了，有几个对象的实例数和占用空间都非常高，其中BizBillDetailBo就是我们上面提到的对象，通过此处直接就定位到问题位置就是我们上面提到的while循环。那些BigDecimal又是什么呢？我们点进去后可以发现，它也是来自于BizBillDetailBo，BigDecimal的实例比较多是因为BizBillDetailBo内部有很多个BigDecimal类型的属性。   
![image](7)    

至此问题得以解决，经过与接口方沟通，在正常的情况下他们不会出现这种数据，但是异常的情况下会有，一旦访问到这种这种数据就会出现死循环。          
这里也警示我们对于一些循环的写法要小心，很容易出错导致死循环，同时这里缺少**防御性编程**的思想，过多信任对方的接口和业务逻辑。       

**问题解决**    
解决的方式很简单，不要出现死循环即可，改后我们看到如下堆内存的使用正常了，gc的次数和时间也都变少了。   
![image](8)
![image](9)

## 总结     
回到我司的那道问题，在dump不下来应用的情况下怎么去分析这个问题。开始我的想法是通过visualvm jmx去远程应用，不过这个首先的服务有开启jmx端口，同时还得解决网络问题。     
其实jmap除了dump文件下来，也支持直接查看分析堆信息了，如下：   
```
jmap -histo:live pid | head -10
```
就可以查看堆中排名前10的实例对象，这个结果和dump出来是一样的，live参数表示我们只看当前存活的对象。      
对于这道题目，我想完整的回答应该是：    
1. 首先将出现问题的节点下线，例如我们使用eureka可以触发客户端主动下线，让节点脱离集群，不影响其它服务的运行。   
2. 重启一个新的节点去顶替它工作，保证整体运行正常。    
3. 分析各种监控数据，看看问题大概出现在哪个位置。   
4. dump文件如果太大就直接上面的jmap命令排查占用较大的前几名，如果能dump下来就dump下来可以图形化分析。    
如果服务支持jmx，也可以想办法通过jmx远程查看堆信息，另外排查程序中的静态变量和大循环，这些可以是加分项，并不是每次都能通过dump文件就直接看出问题所在。    
目前arthas还不支持查看堆中具体对象信息，完整的分析堆中对象信息也是很有必要的，不过这个可能会造成服务不可用，生产上执行要小心。   























